# Study Project: Ethereum Analytics Data Engineering Project

This project is developed to explore and understand key concepts in data engineering and machine learning. The project involves building a pipeline that handles both real-time data and batch processing, applies machine learning for predictions, and visualizes the results.

The goal is to get a hands-on experience with real-world technologies like Kafka, Apache Flink, Airflow, machine learning models, and Grafana. Also gain insight into how these systems work together.

This documentation covers the following components:

-   [Real-Time Streaming Pipeline](#real-time-streaming-pipeline)
-   [Job Scheduling](#job-scheduling-with-airflow)
-   [Data Visualization with Grafana](#data-visualization-with-grafana)

## Real-Time Streaming Pipeline

The real-time streaming pipeline is designed to handle price data from the 0xweb API and blockchain data from Infura. These data streams are consumed, processed, and stored efficiently to support downstream analytics and machine learning models.

**Components**

-   Kafka: Kafka acts as a message broker that captures streaming data from the 0xweb and Infura APIs.
-   Apache Flink: Flink is used to consume, process, and perform transformations on real-time data streams.
-   PostgreSQL: Processed output from the Flink job is stored into a PostgreSQL database.

**Process**

1. Produce Real-Time Price Data from 0xweb to Kafka
    - 0xweb provides real-time price updates for various cryptocurrencies.
    - A Kafka producer is implemented with Python to pull price data from 0xweb API and push it into a Kafka topic.
1. Produce Real-Time Blockchain Data from Infura to Kafka
    - Blockchain data from Infura is produced similarly, using the Infura API to stream Ethereum data (transactions, blocks, events) into Kafka topics.
1. Consume and Process Data with Apache Flink
    - Apache Flink will consume the Kafka topics containing price and blockchain data.
    - Data is processed in real-time, such as filtering, aggregating, and enriching the data to make it more usable for downstream systems.
1. Sink Output to PostgreSQL

## Job Scheduling with Airflow

Apache Airflow is used to orchestrate the execution of batch jobs and to schedule data pipelines, including both daily batch processes and real-time data consumption.

**Components**

-   Google BigQuery: Daily batch data is consumed from a Google BigQuery dataset.
-   Airflow: Used to schedule and manage batch jobs for pre-processing, model predictions, and data storage.
-   PostgreSQL: Stores model prediction results for further analysis.

**Process**

1. Consume Daily Batch Data from Google BigQuery
    - Airflow is configured to trigger jobs that consume batch data from a Google BigQuery dataset, typically on a daily basis. This batch data can contain historical price, blockchain, or other related information.
1. Pre-Process Data
    - The data is pre-processed to clean, filter, and transform it into a format suitable for input into machine learning models.
1. Input Data into CNN-LSTM Model
    - The pre-processed data is passed to a CNN-LSTM model for predictions. This model can be used to predict market sentiment index.
1. Store Prediction Results into PostgreSQL

## Data Visualization with Grafana

Grafana is used for visualizing the data and predictions stored in PostgreSQL, providing real-time and historical insights into the data pipeline, model predictions, and other metrics.

**Components**

-   PostgreSQL Replica: A read-only replica of the PostgreSQL database is created to handle the read load generated by Grafana dashboards.
-   Grafana: An open-source platform for monitoring and observability that consumes data from PostgreSQL and visualizes key metrics and insights.

**Process:**

1. Create PostgreSQL Replica
    - To ensure that Grafana dashboards do not impact the performance of the primary database, a read-only replica of PostgreSQL is set up. This replica handles the querying load generated by Grafana.
1. Consume Data from PostgreSQL Replica
    - Grafana is configured to connect to the PostgreSQL replica, where it can query the necessary tables to generate visualizations, such as time-series graphs, and other analytics metrics.
1. Create Dashboards in Grafana
    - Dashboards are created to display various metrics, including:
        - Real-time cryptocurrency price trends.
        - Blockchain activity metrics.
        - Predictions generated by the CNN-LSTM model.

## Architecture

![Architect](/assets/architect.jpg)

## Dashboard

![Dashboard](/assets/grafana.png)
